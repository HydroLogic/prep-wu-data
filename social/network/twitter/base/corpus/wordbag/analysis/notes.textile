h1. Some notzs fur u

h2. Filter data for training:

The fitting will take forever if your data looks like crap. Don't even consider words that have a count <= 2. This is justified because
our sample is so frickin big. If a word doesn't show up more than twice then it's probably not a word. I don't care how kewl yu think you
are just because you can bash on the keyboard. Also, we should not include wordbags with less than say 50~100 words in training. Finlyy,
training takes forever unless you keep the distribution size (number of trials,success tuples) less than 10k.


Condensed as a list for you to follow:

Remember, we a generating a sample set for our classifier. It's a baby and it needs pure, good, food for thought.

* Throw out all words that show up less than twice (not food!)
* Throw out all users that have wordbags less then 75 words (not nutritious enough!)
* Limit the sample for each word to less than 10k (overwhelming!)

h2. Wat shuwd my training data loook like??

term    {(successes,trials)}

