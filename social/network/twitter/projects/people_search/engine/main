#!/usr/bin/env jruby

require 'rubygems'
require 'swineherd' ; include Swineherd
require 'swineherd/script/pig_script' ; include Swineherd::Script
require 'json'

inputdir  = ARGV[0]
outputdir = ARGV[1]

#
# Get new hadoop file system
#
hdfs = Swineherd::FileSystem.get(:hdfs)

#
# Read in working config file
#
options = YAML.load(hdfs.open(File.join(outputdir, "env", "working_environment.yaml")).read)

#
# Define workflow
#
flow = Swineherd::Workflow.new(options['flow_id']) do
  profile_indexer = PigScript.new(File.dirname(__FILE__)+'/templates/profile_loader.pig.erb')

  task :index_profiles do
    profile_indexer.pig_classpath = options['pig_classpath']
    profile_indexer.attributes = {
      :jars                 => options['es_jars'] + options['hbase_jars'],
      :hbase_table          => options['hbase_table'],
      :hbase_column_family  => options['hbase_column_family'],
      :twitter_user_profile => options['twitter_user_profile'],
      :es_index             => options['es_index'],
      :es_obj_type          => options['es_obj_type'],
      :bulk_size            => 500
    }
    profile_indexer.output << next_output(:index_profiles)
    profile_indexer.run
    hdfs.mkpath(latest_output(:index_profiles)) # so it doesn't run again
  end

end

flow.workdir = File.join(inputdir, "rawd")
flow.describe

#
# Run workflow
#
flow.run :index_profiles
