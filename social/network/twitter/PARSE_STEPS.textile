+ Twitter Parse Steps

++ Top level dependencies:
-- Hadoop cluster set up with all the usual ruby, rubygems, wukong, wuclan, etc. See section on chef?

+++ (1) Run the parse.

++++ Dependencies
None.
++++ Data location
 
The fresh, unparsed and bzipped, (ripd) data lives in the following place:
     -- s3://monkeyshines.infochimps.org/data/ripd/com.tw/
There you will find 3 separate 'directories' corresponding to the 3 different
types of API calls we can make, and thus, 3 different kinds of twitter data
we can scrape. These directories are:
     -- com.twitter.search
     -- com.twitter.stream
     -- com.twitter
Lastly, you'll want to take note of the date the data was last parsed and only
parse data newer than that.

++++ Script location
 
There are 3 scripts, one for each type of API call, for running the parse called:
     -- parse_twitter_search_requests.rb
     -- parse_twitter_stream_requests.rb
     -- parse_twitter_requests.rb

++++ Output location
 
Currently we would like the parsed data to live in the following places on the hdfs:
     -- /data/rawd/social/network/twitter/parsed/search
     -- /data/rawd/social/network/twitter/parsed/stream
     -- /data/rawd/social/network/twitter/parsed/api
If these directories do not already exist you'll want to make them. From the master node run:
[[code]]
hdp-mkdir /data/rawd/social/network/twitter/parsed/search
hdp-mkdir /data/rawd/social/network/twitter/parsed/stream
hdp-mkdir /data/rawd/social/network/twitter/parsed/api
[[/code]]

++++ Run command

Now that we have everything together we can run the parse. From the master node run:
[[code]]
./parse_twitter_search_requests.rb --rm --run  s3://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter.search/date_or_dates_to_parse /data/rawd/social/network/twitter/parsed/search
./parse_twitter_stream_requests.rb --rm --run  s3://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter.stream/date_or_dates_to_parse /data/rawd/social/network/twitter/parsed/stream
./parse_twitter_requests.rb --rm --run  s3://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter/date_or_dates_to_parse /data/rawd/social/network/twitter/parsed/api
[[/code]]
 
That will take reasonably long amount of time (if nothing fails) so you'll want to go do something else...

+++ (2) Split out objects by resource name and uniq immutable objects

The goal of this step is to take the immutable objects (ie. TwitterUserId) we have, both from
previous parses and the current parse, merge them, uniq them, and land them in individual locations.

++++ Dependencies
Step 1.

++++ Data location

There are two places to be concerned with here:
 -- /data/rawd/social/network/twitter/objects
 -- /data/rawd/social/network/twitter/parsed/*

++++ Script location

At the moment there isn't a great script to accomplish this. Until the 'moist unsplicer' can be made to work correctly this will have to
be done using:
 -- <something that needs to be written>
This script can be found in infochimps-data under:
 -- infochimps-data/social/network/twitter/parse_and_scrape_helpers/unsplice_by_grep_uniq/

++++ Output location

Every object should get its own folder under
 -- /data/rawd/social/network/twitter/objects

++++ Run command

Until we have a good script to do this there really isn't a single command to run. Instead, the result of this step should be many directories on the hdfs under:
 -- /data/rawd/social/network/twitter/objects

++++ Example

Take the tweet objects as an example. We will have tweets already existing in:
 -- /data/rawd/social/network/twitter/objects/tweet
First, move this out of the way using hdp-mv in the following way:
[[code]]
hdp-mv /data/rawd/social/network/twitter/objects/tweet /data/rawd/social/network/twitter/parsed/tweet
[[/code]]
Then, using the newly parsed objects and the tweets that have just been moved as inputs, run the following:
[[code]]
 ./unsplice_gu-tweet.sh
[[/code]]
This script will take both /data/rawd/social/network/twitter/parsed/tweet and /data/rawd/social/network/twitter/parsed/*, search them for tweet objects (in the case of the already parsed data every line will contain a tweet object) and uniq them. The data will end up at:
 -- /data/rawd/social/network/twitter/objects/tweet

THIS PART NEEDS WORK!
   
+++ (3) Uniq mutable objects by last seen state

The goal of this step is a set of directories containing all the mutable objects (ie. TwitterUser, TwitterUserPartial, etc) uniqed by last seen state.

++++ Dependencies
Step 1.

++++ Data location 

Here the location should simply be the output from the parse on the hdfs as well as the current mutable object locations:
 -- /data/rawd/social/network/twitter/parsed/*
 -- /data/rawd/social/network/twitter/objects

++++ Script location
 
The script for this lives in infochimps-data under:
 -- infochimps-data/social/network/twitter/parse_and_scrape_helpers/last_seen_state.rb

++++ Output location

The output of this step should be the same as the uniq immutable step:
 -- /data/rawd/social/network/twitter/objects/<resource_name>
Here <resource_name> should be replaced with the mutable objects resource name in question.

++++ Run command

From the master node run:

 THIS PART NEEDS WORK!
 

+++ (4) Extract id objects

The goal of this step is a list of all twitter_user_id objects that we've ever seen. This list will be fed back into the scrapers to make our scraping more efficient.

++++ Dependencies
Step 2
Step 3

++++ Data location

We want to extract twitter_user_id objects from the following objects:
 -- twitter_user_id
 -- twitter_user
 -- twitter_user_partial
 -- tweet
 -- a_follows_b

These objects should be uniq, after the previous steps, and live in:
 -- /data/rawd/social/network/twitter/objects/twitter_user_id
 -- /data/rawd/social/network/twitter/objects/twitter_user
 -- /data/rawd/social/network/twitter/objects/twitter_user_partial
 -- /data/rawd/social/network/twitter/objects/tweet
 -- /data/rawd/social/network/twitter/objects/a_follows_b

++++ Script location
The script to accomplish this is called:
 -- extract_twitter_user_ids_from_twitter_user_anythings.rb
This script is  part of infochimps-data and lives:
 -- infochimps-data/social/network/twitter/parse_and_scrape_helpers

++++ Output location

The output should go:
 -- /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids
We would like to turn this into a list so the output should be streamed into a flat
-- /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids.tsv

++++ Run command

From the master node run:
[[code]]
./extract_twitter_user_ids_from_twitter_user_anythings.rb --rm --run /data/rawd/social/network/twitter/objects/twitter_user_id,/data/rawd/social/network/twitter/objects/twitter_user,/data/rawd/social/network/twitter/objects/twitter_user_partial,/data/rawd/social/network/twitter/objects/a_follows_b,/data/rawd/social/network/twitter/objects/tweet /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids
hdp-catd /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids > /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids.tsv
[[/code]]


+++ (5) Merge API and Search Ids

++++ Dependencies
Step 4
++++ Data location

We have two input locations:
 -- /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids
 -- /data/rawd/social/network/twitter/objects/twitter_user_search_id

++++ Script location

We need to merge the above two types of objects using the following pig script:
 -- match_api_and_search_ids.pig
This script is part of infochimps-data and lives:
 -- infochimps-data/social/network/twitter/parse_and_scrape_helpers

++++ Output location

The data needs to land in the following place:
 -- /data/rawd/social/network/twitter/objects/twitter_user_id_matched

++++ Run command

From the master node run:
[[code]]
pig match_api_and_search_ids.pig
[[/code]]

+++ (6) Get raw requests metadata

++++ Dependencies
None.
++++ Data location

We have two input locations:
 -- s3n://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter/20*/*
 -- s3n://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter.search/20*/*

++++ Script location

The script to accomplish this is called:
 -- raw_requests_metadata.rb
This script is part of infochimps-data and lives:
 -- infochimps-data/social/network/twitter/parse_and_scrape_helpers/request_stats

++++ Output location

The data needs to land in the following place:
 -- /data/rawd/social/network/twitter/scrape_stats/requests_metadata/raw_requests_metadata

++++ Run command

From the master node run:
[[code]]
./raw_requests_metadata.rb --rm --run s3n://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter/20*/*,s3n://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter.search/20\*/\* /data/rawd/social/network/twitter/scrape_stats/requests_metadata/raw_requests_metadata
pig request_stats.pig
[[/code]]

+++ (7) Extract raw token objects

++++ Data location

Here the location should simply be the output from the parse on the hdfs:
 -- /data/rawd/social/network/twitter/parsed/*

++++ Script location
 -- extract_tweet_tokens.rb
 -- infochimps-data/social/network/twitter/parse_and_scrape_helpers

++++ Output location
 -- /data/rawd/social/network/twitter/parsed/tweet_tokens

++++ Run command

 THIS PART NEEDS WORK!

+++ (8) Make data sets

 Go!
