

./extract_twitter_user_ids_from_twitter_user_anythings.rb --rm --run \
  /data/rawd/social/network/twitter/objects/twitter_user\*,/data/rawd/social/network/twitter/objects/a_follows_b,/data/rawd/social/network/twitter/objects/tweet \
  /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids &
hdp-catd /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids > /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids.tsv &



    * /data/rawd/social/network/twitter/objects has uniqd data that includes everything (october parse merged with now).  It's possible that some duplicate fields records exist if their id's were zero-padded in one place and not another. Still need to extract a_favorites_b and twitter_search_id records.
    * parse_and_scrape_helpers/requests_stats/raw_requests_metadata.rb -- the type, request info, page, date, and response code for each request. Data in    /data/rawd/social/network/twitter/scrape_stats/requests_metadata/raw_requests_metadata
    * extract_twitter_user_ids_from_twitter_user_anythings.rb -- look for every api user_id that's ever been seen and merge it into one big list (for feeding back into scrapers later).  Was run over everything but the a_follows_b models. Data in /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids on the HDFS and in /data/rawd/social/network/twitter/scrape_stats/twitter_user_ids.tsv on gibbon master.
    * raw, un-uniqd data is in /data/rawd/social/network/twitter/parsed/\*/\*

* Run the parse.
  --------------
 i. Data location
 
 - the fresh, unparsed and bzipped, (ripd) data lives in the following place:
 -- s3://monkeyshines.infochimps.org/data/ripd/com.tw/
 - there you will find 3 separate 'directories' corresponding to the 3 different
   types of API calls we can make, and thus, 3 different kinds of twitter data
   we can scrape. These directories are:
 -- com.twitter.search
 -- com.twitter.stream
 -- com.twitter
 - lastly, you'll want to take note of the date the data was last parsed and only
   parse data newer than that

 ii. Script location
 
 - there are 3 scripts, one for each type of API call, for running the parse called:
 -- parse_twitter_search_requests.rb
 -- parse_twitter_stream_requests.rb
 -- parse_twitter_requests.rb

 iii. Output location
 
 - currently we would like the parsed data to live in the following places on the hdfs:
 -- /data/rawd/social/network/twitter/parsed/search
 -- /data/rawd/social/network/twitter/parsed/stream
 -- /data/rawd/social/network/twitter/parsed/api
 - if these directories do not already exist you'll want to make them. from the cluster:
 -- hdp-mkdir /data/rawd/social/network/twitter/parsed/search
 -- hdp-mkdir /data/rawd/social/network/twitter/parsed/stream
 -- hdp-mkdir /data/rawd/social/network/twitter/parsed/api

 iv. Run command

 - now that we have everything together we can run the parse. from the master node run:
 
 -- ./parse_twitter_search_requests.rb --rm --run  s3://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter.search/date_or_dates_to_parse /data/rawd/social/network/twitter/parsed/search
 -- ./parse_twitter_stream_requests.rb --rm --run  s3://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter.stream/date_or_dates_to_parse /data/rawd/social/network/twitter/parsed/stream
 -- ./parse_twitter_requests.rb --rm --run  s3://monkeyshines.infochimps.org/data/ripd/com.tw/com.twitter/date_or_dates_to_parse /data/rawd/social/network/twitter/parsed/api
 
 - that will take reasonably long amount of time (if nothing fails) so you'll want to go do something else...
